services:
  # vLLM Server - Provides LLM inference service (OpenAI API compatible interface)
  # According to official docs: https://docs.vllm.ai/en/stable/getting_started/installation/cpu.html
  # Note:
  # - On macOS (ARM64), need platform: linux/amd64 to run x86_64 containers via Rosetta 2
  # - On Linux x86_64, no platform restriction needed, use native architecture
  vllm-server:
    # platform: linux/amd64  # Only needed on macOS, comment out on Linux x86_64
    build:
      context: .
      dockerfile: Dockerfile.vllm
      args:
        BUILD_PROXY: ${BUILD_PROXY:-}
    image: vllm-server:latest
    ports:
      - "8001:8001"
    volumes:
      - ./models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
      # Force CPU mode (disable GPU)
      - CUDA_VISIBLE_DEVICES=""
      - VLLM_TARGET_DEVICE=cpu
      # vLLM CPU configuration (according to official docs)
      - VLLM_CPU_KVCACHE_SPACE=2  # KV Cache space (GB), reduced to 2GB to reduce memory usage
      - VLLM_CPU_OMP_THREADS_BIND=auto  # Auto bind CPU cores
      - VLLM_CPU_NUM_OF_RESERVED_CPU=1  # Reserve 1 CPU core for framework
      - VLLM_LOGGING_LEVEL=INFO  # Use INFO level for normal operation
      # Model path (local path in container)
      - VLLM_MODEL_NAME=${VLLM_MODEL_NAME:-/app/models/qwen2.5-1.5b-instruct}
      # HuggingFace offline mode, force using local files, avoid network download
      - HF_HUB_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
    # Docker permission configuration (according to official docs, solve NUMA-related permission issues)
    security_opt:
      - seccomp=unconfined
    cap_add:
      - SYS_NICE
    shm_size: 6g  # Shared memory size, increased to 6GB to support inter-process communication
    # Memory limits: ensure sufficient memory for model, KV cache and system overhead
    # Model 2.9GB + KV cache 4GB + system overhead 1GB + buffer 3GB = ~11GB
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          memory: 10G
    # Auto-start vLLM server using startup script
    # Model path is read from VLLM_MODEL_NAME environment variable by the script
    command: ["/app/start_vllm_server.sh", "8001", "0.0.0.0"]
    restart: unless-stopped
    networks:
      - vllm-langchain-network
    # If GPU support is needed, uncomment below and remove CPU-related configuration
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # FastAPI Chat Server - Provides chat service
  chat-server:
    build:
      context: .
      args:
        BUILD_PROXY: ${BUILD_PROXY:-}
    image: vllm-langchain-demo:latest
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
      - VLLM_SERVER_URL=http://vllm-server:8001/v1
      - VLLM_MODEL_NAME=${VLLM_MODEL_NAME:-/app/models/qwen2.5-1.5b-instruct}
      # Clear potentially leftover proxy environment variables (avoid affecting inter-container communication)
      - http_proxy=
      - https_proxy=
      - HTTP_PROXY=
      - HTTPS_PROXY=
    command: /app/.venv/bin/python chat_server.py
    depends_on:
      - vllm-server
    restart: unless-stopped
    networks:
      - vllm-langchain-network

networks:
  vllm-langchain-network:
    driver: bridge

