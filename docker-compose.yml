services:
  # vLLM服务器 - 提供LLM推理服务（OpenAI API兼容接口）
  # 根据官方文档：https://docs.vllm.ai/en/stable/getting_started/installation/cpu.html
  # 注意：
  # - 在 macOS (ARM64) 上，需要 platform: linux/amd64 通过 Rosetta 2 运行 x86_64 容器
  # - 在 Linux x86_64 上，不需要 platform 限制，使用原生架构
  vllm-server:
    # platform: linux/amd64  # 仅在 macOS 上需要，Linux x86_64 上注释掉
    build:
      context: .
      dockerfile: Dockerfile.vllm
      args:
        BUILD_PROXY: ${BUILD_PROXY:-}
    image: vllm-server:latest
    ports:
      - "8001:8001"
    volumes:
      - ./models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
      # vLLM CPU 配置（根据官方文档）
      - VLLM_CPU_KVCACHE_SPACE=4  # KV Cache 空间（GB），默认 4GB
      - VLLM_CPU_OMP_THREADS_BIND=auto  # 自动绑定 CPU 核心
      - VLLM_CPU_NUM_OF_RESERVED_CPU=1  # 为框架保留 1 个 CPU 核心
    # Docker 权限配置（根据官方文档，解决 NUMA 相关权限问题）
    security_opt:
      - seccomp=unconfined
    cap_add:
      - SYS_NICE
    shm_size: 4g  # 共享内存大小
    # 启动 vLLM OpenAI API 服务器（CPU 模式）
    # 根据官方文档，使用 vllm serve 命令，并指定 dtype=bfloat16
    command: >
      python -m vllm.entrypoints.openai.api_server
      --model ${VLLM_MODEL_NAME:-meta-llama/Llama-3.1-8B-Instruct}
      --port 8001
      --host 0.0.0.0
      --trust-remote-code
      --dtype bfloat16
    restart: unless-stopped
    networks:
      - vllm-langchain-network
    # 如果需要 GPU 支持，取消下面的注释并移除 CPU 相关配置
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # FastAPI Chat服务器 - 提供聊天服务
  chat-server:
    build:
      context: .
      args:
        BUILD_PROXY: ${BUILD_PROXY:-}
    image: vllm-langchain-demo:latest
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
      - VLLM_SERVER_URL=http://vllm-server:8001/v1
      - VLLM_MODEL_NAME=${VLLM_MODEL_NAME:-meta-llama/Llama-3.1-8B-Instruct}
      # 清除可能残留的代理环境变量（避免影响容器间通信）
      - http_proxy=
      - https_proxy=
      - HTTP_PROXY=
      - HTTPS_PROXY=
    command: /app/.venv/bin/python chat_server.py
    depends_on:
      - vllm-server
    restart: unless-stopped
    networks:
      - vllm-langchain-network

networks:
  vllm-langchain-network:
    driver: bridge

