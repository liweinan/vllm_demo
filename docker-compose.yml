services:
  # vLLM Server - Provides LLM inference service (OpenAI API compatible interface)
  # According to official docs: https://docs.vllm.ai/en/stable/getting_started/installation/cpu.html
  # Note:
  # - On macOS (ARM64), need platform: linux/amd64 to run x86_64 containers via Rosetta 2
  # - On Linux x86_64, no platform restriction needed, use native architecture
  vllm-server:
    # platform: linux/amd64  # Only needed on macOS, comment out on Linux x86_64
    build:
      context: .
      dockerfile: Dockerfile.vllm
      args:
        BUILD_PROXY: ${BUILD_PROXY:-}
    image: vllm-server:latest
    ports:
      - "8001:8001"
    volumes:
      - ./models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
      # Force CPU mode (disable GPU)
      - CUDA_VISIBLE_DEVICES=""
      - VLLM_TARGET_DEVICE=cpu
      # vLLM CPU configuration (according to official docs)
      - VLLM_CPU_KVCACHE_SPACE=2  # KV Cache space (GB), reduced to 2GB to reduce memory usage
      - VLLM_CPU_OMP_THREADS_BIND=auto  # Auto bind CPU cores
      - VLLM_CPU_NUM_OF_RESERVED_CPU=1  # Reserve 1 CPU core for framework
      - VLLM_LOGGING_LEVEL=DEBUG  # Use DEBUG level for platform detection diagnostics
      # HuggingFace offline mode, force using local files, avoid network download
      - HF_HUB_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
    # Docker permission configuration (according to official docs, solve NUMA-related permission issues)
    security_opt:
      - seccomp=unconfined
    cap_add:
      - SYS_NICE
    shm_size: 6g  # Shared memory size, increased to 6GB to support inter-process communication
    # Memory limits: ensure sufficient memory for model, KV cache and system overhead
    # Model 2.9GB + KV cache 4GB + system overhead 1GB + buffer 3GB = ~11GB
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          memory: 10G
    # Don't auto-start service, convenient for manual container debugging
    # Override base image entrypoint, use bash to keep container running
    entrypoint: ["/bin/bash"]
    stdin_open: true  # Keep STDIN open
    tty: true  # Allocate pseudo-TTY
    # After entering container, can execute:
    #   /app/start_vllm_server.sh /app/models/qwen2.5-1.5b-instruct 8001 0.0.0.0
    # Or directly:
    #   python -m vllm.entrypoints.openai.api_server /app/models/qwen2.5-1.5b-instruct --port 8001 --host 0.0.0.0 --trust-remote-code --dtype bfloat16
    restart: unless-stopped
    networks:
      - vllm-langchain-network
    # If GPU support is needed, uncomment below and remove CPU-related configuration
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # FastAPI Chat Server - Provides chat service
  chat-server:
    build:
      context: .
      args:
        BUILD_PROXY: ${BUILD_PROXY:-}
    image: vllm-langchain-demo:latest
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
      - VLLM_SERVER_URL=http://vllm-server:8001/v1
      - VLLM_MODEL_NAME=${VLLM_MODEL_NAME:-Qwen/Qwen2.5-1.5B-Instruct}
      # Clear potentially leftover proxy environment variables (avoid affecting inter-container communication)
      - http_proxy=
      - https_proxy=
      - HTTP_PROXY=
      - HTTPS_PROXY=
    command: /app/.venv/bin/python chat_server.py
    depends_on:
      - vllm-server
    restart: unless-stopped
    networks:
      - vllm-langchain-network

networks:
  vllm-langchain-network:
    driver: bridge

