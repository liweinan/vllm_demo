# vLLM CPU 服务 Dockerfile
# 根据官方文档：https://docs.vllm.ai/en/stable/getting_started/installation/cpu.html
# 
# 方案1：使用本地构建的 vLLM CPU 镜像（推荐，如果已构建）
FROM vllm-cpu-env:latest

# 方案2：从源码构建（如果本地镜像不可用，取消下面的注释并注释掉上面的 FROM）
# FROM python:3.11-slim
# ARG BUILD_PROXY
# RUN apt-get update && apt-get install -y --no-install-recommends \
#     gcc g++ libnuma-dev python3-dev git cmake build-essential \
#     && rm -rf /var/lib/apt/lists/*
# WORKDIR /app
# RUN git clone https://github.com/vllm-project/vllm.git vllm_source && \
#     cd vllm_source && \
#     pip install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu && \
#     pip install --no-cache-dir -r requirements/cpu-build.txt && \
#     pip install --no-cache-dir -r requirements/cpu.txt && \
#     VLLM_TARGET_DEVICE=cpu pip install . --no-build-isolation && \
#     cd .. && rm -rf vllm_source

# 设置工作目录
WORKDIR /app

# 创建模型目录
RUN mkdir -p /app/models

# 暴露端口
EXPOSE 8001

# 默认启动命令（可以通过 docker-compose.yml 覆盖）
CMD ["python", "-m", "vllm.entrypoints.openai.api_server", "--host", "0.0.0.0", "--port", "8001"]
